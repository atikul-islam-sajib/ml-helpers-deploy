<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Custom RF Regressor - ML-Helpers</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Custom RF Regressor";
        var mkdocs_page_input_path = "regressor.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> ML-Helpers
        </a>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../benchmark_evalaute/">Benchmark Evaluate</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../charts/">Charts</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../classifier/">Custom RF Classifier</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Custom RF Regressor</a>
    <ul class="current">
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../utils/">Utils</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">ML-Helpers</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Custom RF Regressor</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-module">



<a id="regressor"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="regressor.CustomRandomForestRegressor" class="doc doc-heading">
          <code>CustomRandomForestRegressor</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="sklearn.ensemble.RandomForestRegressor">RandomForestRegressor</span></code></p>

  
      <p>A custom implementation of RandomForestRegressor that supports weighting trees based on their
out-of-bag (OOB) error.</p>
<p>This class extends sklearn's RandomForestRegressor, adding the functionality to compute and use
weights for each tree in the ensemble. Weights are derived from the exponential of the negative
OOB error, enabling more influential contributions from better-performing trees when making predictions.</p>
<p>Inherits from <code>sklearn.ensemble.RandomForestClassifier</code>:</p>
<pre><code class="language-python">class sklearn.ensemble.RandomForestRegressor(
    n_estimators=100, *, criterion='squared_error', max_depth=None,
    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,
    max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True,
    oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0,
    max_samples=None, monotonic_cst=None)
</code></pre>
<h4 id="regressor.CustomRandomForestRegressor--parameters">Parameters:</h4>
<p>All parameters of the sklearn.ensemble.RandomForestRegressor class are accepted.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>n_estimators</code></b>
              –
              <div class="doc-md-description">
                <p>int, default=100
The number of trees in the forest.</p>
              </div>
            </li>
            <li>
              <b><code>criterion</code></b>
              –
              <div class="doc-md-description">
                <p>{“squared_error”, “absolute_error”, “friedman_mse”, “poisson”}, default="“squared_error”"
The function to measure the quality of a split.</p>
              </div>
            </li>
            <li>
              <b><code>max_depth</code></b>
              –
              <div class="doc-md-description">
                <p>int, default=None
The maximum depth of the tree.</p>
              </div>
            </li>
            <li>
              <b><code>min_samples_split</code></b>
              –
              <div class="doc-md-description">
                <p>int or float, default=2
The minimum number of samples required to split an internal node.</p>
              </div>
            </li>
            <li>
              <b><code>min_samples_leaf</code></b>
              –
              <div class="doc-md-description">
                <p>int or float, default=1
The minimum number of samples required to be at a leaf node.</p>
              </div>
            </li>
            <li>
              <b><code>min_weight_fraction_leaf</code></b>
              –
              <div class="doc-md-description">
                <p>float, default=0.0
The minimum weighted fraction of the sum total of weights required to be at a leaf node.</p>
              </div>
            </li>
            <li>
              <b><code>max_features</code></b>
              –
              <div class="doc-md-description">
                <p>{"sqrt", "log2", None}, int or float, default="sqrt"
The number of features to consider when looking for the best split.</p>
              </div>
            </li>
            <li>
              <b><code>bootstrap</code></b>
              –
              <div class="doc-md-description">
                <p>bool, default=True
Whether bootstrap samples are used when building trees.</p>
              </div>
            </li>
            <li>
              <b><code>oob_score</code></b>
              –
              <div class="doc-md-description">
                <p>bool, default=False
Whether to use out-of-bag samples to estimate the generalization accuracy.</p>
              </div>
            </li>
            <li>
              <b><code>n_jobs</code></b>
              –
              <div class="doc-md-description">
                <p>int, default=None
The number of jobs to run in parallel for both <code>fit</code> and <code>predict</code>.</p>
              </div>
            </li>
            <li>
              <b><code>random_state</code></b>
              –
              <div class="doc-md-description">
                <p>int, RandomState instance or None, default=None
Controls both the randomness of the bootstrapping of the samples used when building trees (if <code>bootstrap=True</code>) and the sampling of the features to consider when looking for the best split at each node (if <code>max_features &lt; n_features</code>).</p>
              </div>
            </li>
            <li>
              <b><code>verbose</code></b>
              –
              <div class="doc-md-description">
                <p>int, default=0
Controls the verbosity when fitting and predicting.</p>
              </div>
            </li>
            <li>
              <b><code>warm_start</code></b>
              –
              <div class="doc-md-description">
                <p>bool, default=False
When set to <code>True</code>, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.</p>
              </div>
            </li>
            <li>
              <b><code>ccp_alpha</code></b>
              –
              <div class="doc-md-description">
                <p>non-negative float, default=0.0
Complexity parameter used for Minimal Cost-Complexity Pruning.</p>
              </div>
            </li>
            <li>
              <b><code>max_samples</code></b>
              –
              <div class="doc-md-description">
                <p>int or float, default=None
The number of samples to draw from X to train each base estimator.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>      <h4 id="regressor.CustomRandomForestRegressor--attributes">Attributes:</h4>
<ul>
<li><code>in_bag_indices_</code> : list of arrays
    Indices of samples drawn for training each tree.</li>
<li><code>oob_indices_</code> : list of arrays
    Out-of-bag sample indices for each tree.</li>
<li><code>tree_weights_</code> : list of floats
    Weights for each tree, computed based on their OOB error.</li>
</ul>
<h4 id="regressor.CustomRandomForestRegressor--methods">Methods:</h4>
<ul>
<li><code>fit(X, y)</code>: Fits the random forest regressor model on the input data <code>X</code> and target <code>y</code>.</li>
<li><code>predict(X, weights=None)</code>: Predicts regression target for <code>X</code>. The <code>weights</code> parameter can
be either 'uniform' or 'expOOB' to influence prediction.</li>
</ul>
<h4 id="regressor.CustomRandomForestRegressor--examples">Examples:</h4>
<pre><code class="language-python">from sklearn.datasets import make_regression

X, y = make_regression(n_samples=1000, n_features=4, n_informative=2, noise=0.5, random_state=42)
reg = CustomRandomForestRegressor(n_estimators=100, random_state=42)
reg.fit(X, y)
y_pred = reg.predict(X[:5])
</code></pre>

            <details class="quote">
              <summary>Source code in <code>regressor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CustomRandomForestRegressor</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A custom implementation of RandomForestRegressor that supports weighting trees based on their</span>
<span class="sd">    out-of-bag (OOB) error.</span>

<span class="sd">    This class extends sklearn&#39;s RandomForestRegressor, adding the functionality to compute and use</span>
<span class="sd">    weights for each tree in the ensemble. Weights are derived from the exponential of the negative</span>
<span class="sd">    OOB error, enabling more influential contributions from better-performing trees when making predictions.</span>

<span class="sd">    Inherits from `sklearn.ensemble.RandomForestClassifier`:</span>
<span class="sd">    ```python</span>
<span class="sd">    class sklearn.ensemble.RandomForestRegressor(</span>
<span class="sd">        n_estimators=100, *, criterion=&#39;squared_error&#39;, max_depth=None,</span>
<span class="sd">        min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,</span>
<span class="sd">        max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True,</span>
<span class="sd">        oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0,</span>
<span class="sd">        max_samples=None, monotonic_cst=None)</span>
<span class="sd">    ```</span>


<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    All parameters of the sklearn.ensemble.RandomForestRegressor class are accepted.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        n_estimators : int, default=100</span>
<span class="sd">            The number of trees in the forest.</span>
<span class="sd">        criterion : {“squared_error”, “absolute_error”, “friedman_mse”, “poisson”}, default=&quot;“squared_error”&quot;</span>
<span class="sd">            The function to measure the quality of a split.</span>
<span class="sd">        max_depth : int, default=None</span>
<span class="sd">            The maximum depth of the tree.</span>
<span class="sd">        min_samples_split : int or float, default=2</span>
<span class="sd">            The minimum number of samples required to split an internal node.</span>
<span class="sd">        min_samples_leaf : int or float, default=1</span>
<span class="sd">            The minimum number of samples required to be at a leaf node.</span>
<span class="sd">        min_weight_fraction_leaf : float, default=0.0</span>
<span class="sd">            The minimum weighted fraction of the sum total of weights required to be at a leaf node.</span>
<span class="sd">        max_features : {&quot;sqrt&quot;, &quot;log2&quot;, None}, int or float, default=&quot;sqrt&quot;</span>
<span class="sd">            The number of features to consider when looking for the best split.</span>
<span class="sd">        bootstrap : bool, default=True</span>
<span class="sd">            Whether bootstrap samples are used when building trees.</span>
<span class="sd">        oob_score : bool, default=False</span>
<span class="sd">            Whether to use out-of-bag samples to estimate the generalization accuracy.</span>
<span class="sd">        n_jobs : int, default=None</span>
<span class="sd">            The number of jobs to run in parallel for both `fit` and `predict`.</span>
<span class="sd">        random_state : int, RandomState instance or None, default=None</span>
<span class="sd">            Controls both the randomness of the bootstrapping of the samples used when building trees (if `bootstrap=True`) and the sampling of the features to consider when looking for the best split at each node (if `max_features &lt; n_features`).</span>
<span class="sd">        verbose : int, default=0</span>
<span class="sd">            Controls the verbosity when fitting and predicting.</span>
<span class="sd">        warm_start : bool, default=False</span>
<span class="sd">            When set to `True`, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.</span>
<span class="sd">        ccp_alpha : non-negative float, default=0.0</span>
<span class="sd">            Complexity parameter used for Minimal Cost-Complexity Pruning.</span>
<span class="sd">        max_samples : int or float, default=None</span>
<span class="sd">            The number of samples to draw from X to train each base estimator.</span>

<span class="sd">    Attributes:</span>
<span class="sd">    -----------</span>
<span class="sd">    - `in_bag_indices_` : list of arrays</span>
<span class="sd">        Indices of samples drawn for training each tree.</span>
<span class="sd">    - `oob_indices_` : list of arrays</span>
<span class="sd">        Out-of-bag sample indices for each tree.</span>
<span class="sd">    - `tree_weights_` : list of floats</span>
<span class="sd">        Weights for each tree, computed based on their OOB error.</span>


<span class="sd">    Methods:</span>
<span class="sd">    --------</span>
<span class="sd">    - `fit(X, y)`: Fits the random forest regressor model on the input data `X` and target `y`.</span>
<span class="sd">    - `predict(X, weights=None)`: Predicts regression target for `X`. The `weights` parameter can</span>
<span class="sd">    be either &#39;uniform&#39; or &#39;expOOB&#39; to influence prediction.</span>

<span class="sd">    Examples:</span>
<span class="sd">    ---------</span>
<span class="sd">    ```python</span>
<span class="sd">    from sklearn.datasets import make_regression</span>

<span class="sd">    X, y = make_regression(n_samples=1000, n_features=4, n_informative=2, noise=0.5, random_state=42)</span>
<span class="sd">    reg = CustomRandomForestRegressor(n_estimators=100, random_state=42)</span>
<span class="sd">    reg.fit(X, y)</span>
<span class="sd">    y_pred = reg.predict(X[:5])</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build a forest of trees from the training set (X, y).</span>

<span class="sd">        Parameters:</span>
<span class="sd">            X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">                The training input samples.</span>
<span class="sd">            y : array-like of shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">                The target values (real numbers).</span>

<span class="sd">        Returns: </span>
<span class="sd">            self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_bag_indices_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">oob_indices_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
            <span class="n">random_state</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">random_state</span>
            <span class="n">in_bag_indices</span> <span class="o">=</span> <span class="n">generate_sample_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
            <span class="n">oob_indices</span> <span class="o">=</span> <span class="n">generate_unsampled_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">in_bag_indices_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">in_bag_indices</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_indices_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">oob_indices</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">oob_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">oob_predictions</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">oob_indices</span><span class="p">])</span>
                <span class="n">oob_loss</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">oob_indices</span><span class="p">],</span> <span class="n">oob_predictions</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">oob_loss</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Normalize tree weights</span>
        <span class="n">total_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">total_weight</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">weight</span> <span class="o">/</span> <span class="n">total_weight</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span>
            <span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict regression target for X using the trained forest.</span>

<span class="sd">        Parameters:</span>
<span class="sd">            weights : {&#39;uniform&#39;, &#39;expOOB&#39;}, default=&#39;uniform&#39;</span>
<span class="sd">                The weighting scheme to use for aggregating predictions from the individual trees.</span>
<span class="sd">                - &#39;uniform&#39;: All trees contribute equally to the final prediction.</span>
<span class="sd">                - &#39;expOOB&#39;: Trees are weighted based on the exponential of the negative out-of-bag error.</span>

<span class="sd">        Returns:</span>
<span class="sd">            y : ndarray of shape = [n_samples]. The predicted values.</span>

<span class="sd">        Notes:</span>
<span class="sd">            If &#39;expOOB&#39; weighting is used, trees with lower out-of-bag error have a greater influence on</span>
<span class="sd">            the final prediction, potentially improving predictive performance on unseen data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;estimators_&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The forest is not fitted yet!&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">weights</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;expOOB&quot;</span><span class="p">,</span> <span class="s2">&quot;uniform&quot;</span><span class="p">]:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="s2">&quot;uniform&quot;</span>

        <span class="c1"># Collect predictions from each tree</span>
        <span class="n">all_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;expOOB&quot;</span><span class="p">:</span>
            <span class="c1"># Use the exponential of the negative out-of-bag error as weights</span>
            <span class="n">weighted_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
            <span class="c1"># All trees have equal weight</span>
            <span class="n">weighted_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">weighted_preds</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="regressor.CustomRandomForestRegressor.fit" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Build a forest of trees from the training set (X, y).</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>X</code></b>
              –
              <div class="doc-md-description">
                <p>array-like or sparse matrix of shape = [n_samples, n_features]
The training input samples.</p>
              </div>
            </li>
            <li>
              <b><code>y</code></b>
              –
              <div class="doc-md-description">
                <p>array-like of shape = [n_samples] or [n_samples, n_outputs]
The target values (real numbers).</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
<b><code>self</code></b>            –
            <div class="doc-md-description">
              <p>object</p>
            </div>
          </li>
          <li>
            –
            <div class="doc-md-description">
              <p>Returns self.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
          <details class="quote">
            <summary>Source code in <code>regressor.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build a forest of trees from the training set (X, y).</span>

<span class="sd">    Parameters:</span>
<span class="sd">        X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples.</span>
<span class="sd">        y : array-like of shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">            The target values (real numbers).</span>

<span class="sd">    Returns: </span>
<span class="sd">        self : object</span>
<span class="sd">        Returns self.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_bag_indices_</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">oob_indices_</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
        <span class="n">random_state</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">random_state</span>
        <span class="n">in_bag_indices</span> <span class="o">=</span> <span class="n">generate_sample_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="n">oob_indices</span> <span class="o">=</span> <span class="n">generate_unsampled_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">in_bag_indices_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">in_bag_indices</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">oob_indices_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">oob_indices</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">oob_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">oob_predictions</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">oob_indices</span><span class="p">])</span>
            <span class="n">oob_loss</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">oob_indices</span><span class="p">],</span> <span class="n">oob_predictions</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">oob_loss</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Normalize tree weights</span>
    <span class="n">total_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">total_weight</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">weight</span> <span class="o">/</span> <span class="n">total_weight</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span>
        <span class="p">]</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="regressor.CustomRandomForestRegressor.predict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Predict regression target for X using the trained forest.</p>



<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
      <th class="field-name">Parameters:</th>
      <td class="field-body">
        <ul class="first simple">
            <li>
              <b><code>weights</code></b>
              –
              <div class="doc-md-description">
                <p>{'uniform', 'expOOB'}, default='uniform'
The weighting scheme to use for aggregating predictions from the individual trees.
- 'uniform': All trees contribute equally to the final prediction.
- 'expOOB': Trees are weighted based on the exponential of the negative out-of-bag error.</p>
              </div>
            </li>
        </ul>
      </td>
    </tr>
  </tbody>
</table>


<table class="field-list">
  <colgroup>
    <col class="field-name" />
    <col class="field-body" />
  </colgroup>
  <tbody valign="top">
    <tr class="field">
    <th class="field-name">Returns:</th>
    <td class="field-body">
      <ul class="first simple">
          <li>
<b><code>y</code></b>            –
            <div class="doc-md-description">
              <p>ndarray of shape = [n_samples]. The predicted values.</p>
            </div>
          </li>
      </ul>
    </td>
    </tr>
  </tbody>
</table>
<details class="notes" open>
  <summary>Notes</summary>
  <p>If 'expOOB' weighting is used, trees with lower out-of-bag error have a greater influence on
the final prediction, potentially improving predictive performance on unseen data.</p>
</details>
          <details class="quote">
            <summary>Source code in <code>regressor.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predict regression target for X using the trained forest.</span>

<span class="sd">    Parameters:</span>
<span class="sd">        weights : {&#39;uniform&#39;, &#39;expOOB&#39;}, default=&#39;uniform&#39;</span>
<span class="sd">            The weighting scheme to use for aggregating predictions from the individual trees.</span>
<span class="sd">            - &#39;uniform&#39;: All trees contribute equally to the final prediction.</span>
<span class="sd">            - &#39;expOOB&#39;: Trees are weighted based on the exponential of the negative out-of-bag error.</span>

<span class="sd">    Returns:</span>
<span class="sd">        y : ndarray of shape = [n_samples]. The predicted values.</span>

<span class="sd">    Notes:</span>
<span class="sd">        If &#39;expOOB&#39; weighting is used, trees with lower out-of-bag error have a greater influence on</span>
<span class="sd">        the final prediction, potentially improving predictive performance on unseen data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;estimators_&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The forest is not fitted yet!&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">weights</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;expOOB&quot;</span><span class="p">,</span> <span class="s2">&quot;uniform&quot;</span><span class="p">]:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="s2">&quot;uniform&quot;</span>

    <span class="c1"># Collect predictions from each tree</span>
    <span class="n">all_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;expOOB&quot;</span><span class="p">:</span>
        <span class="c1"># Use the exponential of the negative out-of-bag error as weights</span>
        <span class="n">weighted_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
        <span class="c1"># All trees have equal weight</span>
        <span class="n">weighted_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">weighted_preds</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>




  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../classifier/" class="btn btn-neutral float-left" title="Custom RF Classifier"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../utils/" class="btn btn-neutral float-right" title="Utils">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../classifier/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../utils/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
