<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Custom RF Regressor - ML-Helpers</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Custom RF Regressor";
        var mkdocs_page_input_path = "regressor.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> ML-Helpers
        </a>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../benchmark_evalaute/">Benchmark Evaluate</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../charts/">Charts</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../classifier/">Custom RF Classifier</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Custom RF Regressor</a>
    <ul class="current">
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../utils/">Utils</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">ML-Helpers</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Custom RF Regressor</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-module">



<a id="regressor"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="regressor.CustomRandomForestRegressor" class="doc doc-heading">
          <code>CustomRandomForestRegressor</code>


</h2>


  <div class="doc doc-contents ">
          <p class="doc doc-class-bases">
            Bases: <code><span title="sklearn.ensemble.RandomForestRegressor">RandomForestRegressor</span></code></p>

  
      <p>A custom implementation of RandomForestRegressor that supports weighting trees based on their
out-of-bag (OOB) error.</p>
<p>This class extends sklearn's RandomForestRegressor, adding the functionality to compute and use
weights for each tree in the ensemble. Weights are derived from the exponential of the negative
OOB error, enabling more influential contributions from better-performing trees when making predictions.</p>
<h4 id="regressor.CustomRandomForestRegressor--parameters">Parameters:</h4>
<p>All parameters of the sklearn.ensemble.RandomForestRegressor class are accepted.</p>
<h4 id="regressor.CustomRandomForestRegressor--attributes">Attributes:</h4>
<ul>
<li><code>in_bag_indices_</code> : list of arrays
    Indices of samples drawn for training each tree.</li>
<li><code>oob_indices_</code> : list of arrays
    Out-of-bag sample indices for each tree.</li>
<li><code>tree_weights_</code> : list of floats
    Weights for each tree, computed based on their OOB error.</li>
</ul>
<h4 id="regressor.CustomRandomForestRegressor--methods">Methods:</h4>
<ul>
<li><code>fit(X, y)</code>: Fits the random forest regressor model on the input data <code>X</code> and target <code>y</code>.</li>
<li><code>predict(X, weights=None)</code>: Predicts regression target for <code>X</code>. The <code>weights</code> parameter can
be either 'uniform' or 'expOOB' to influence prediction.</li>
</ul>
<h4 id="regressor.CustomRandomForestRegressor--examples">Examples:</h4>
<pre><code class="language-python">from sklearn.datasets import make_regression

X, y = make_regression(n_samples=1000, n_features=4, n_informative=2, noise=0.5, random_state=42)
reg = CustomRandomForestRegressor(n_estimators=100, random_state=42)
reg.fit(X, y)
y_pred = reg.predict(X[:5])
</code></pre>

            <details class="quote">
              <summary>Source code in <code>regressor.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">CustomRandomForestRegressor</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A custom implementation of RandomForestRegressor that supports weighting trees based on their</span>
<span class="sd">    out-of-bag (OOB) error.</span>

<span class="sd">    This class extends sklearn&#39;s RandomForestRegressor, adding the functionality to compute and use</span>
<span class="sd">    weights for each tree in the ensemble. Weights are derived from the exponential of the negative</span>
<span class="sd">    OOB error, enabling more influential contributions from better-performing trees when making predictions.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    All parameters of the sklearn.ensemble.RandomForestRegressor class are accepted.</span>

<span class="sd">    Attributes:</span>
<span class="sd">    -----------</span>
<span class="sd">    - `in_bag_indices_` : list of arrays</span>
<span class="sd">        Indices of samples drawn for training each tree.</span>
<span class="sd">    - `oob_indices_` : list of arrays</span>
<span class="sd">        Out-of-bag sample indices for each tree.</span>
<span class="sd">    - `tree_weights_` : list of floats</span>
<span class="sd">        Weights for each tree, computed based on their OOB error.</span>

<span class="sd">    Methods:</span>
<span class="sd">    --------</span>
<span class="sd">    - `fit(X, y)`: Fits the random forest regressor model on the input data `X` and target `y`.</span>
<span class="sd">    - `predict(X, weights=None)`: Predicts regression target for `X`. The `weights` parameter can</span>
<span class="sd">    be either &#39;uniform&#39; or &#39;expOOB&#39; to influence prediction.</span>

<span class="sd">    Examples:</span>
<span class="sd">    ---------</span>
<span class="sd">    ```python</span>
<span class="sd">    from sklearn.datasets import make_regression</span>

<span class="sd">    X, y = make_regression(n_samples=1000, n_features=4, n_informative=2, noise=0.5, random_state=42)</span>
<span class="sd">    reg = CustomRandomForestRegressor(n_estimators=100, random_state=42)</span>
<span class="sd">    reg.fit(X, y)</span>
<span class="sd">    y_pred = reg.predict(X[:5])</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build a forest of trees from the training set (X, y).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">            The training input samples.</span>
<span class="sd">        y : array-like of shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">            The target values (real numbers).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : object</span>
<span class="sd">            Returns self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_bag_indices_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">oob_indices_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
            <span class="n">random_state</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">random_state</span>
            <span class="n">in_bag_indices</span> <span class="o">=</span> <span class="n">generate_sample_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
            <span class="n">oob_indices</span> <span class="o">=</span> <span class="n">generate_unsampled_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">in_bag_indices_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">in_bag_indices</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">oob_indices_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">oob_indices</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">oob_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">oob_predictions</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">oob_indices</span><span class="p">])</span>
                <span class="n">oob_loss</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">oob_indices</span><span class="p">],</span> <span class="n">oob_predictions</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">oob_loss</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Normalize tree weights</span>
        <span class="n">total_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">total_weight</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">weight</span> <span class="o">/</span> <span class="n">total_weight</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span>
            <span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict regression target for X using the trained forest.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">            The input samples.</span>
<span class="sd">        weights : {&#39;uniform&#39;, &#39;expOOB&#39;}, default=&#39;uniform&#39;</span>
<span class="sd">            The weighting scheme to use for aggregating predictions from the individual trees.</span>
<span class="sd">            - &#39;uniform&#39;: All trees contribute equally to the final prediction.</span>
<span class="sd">            - &#39;expOOB&#39;: Trees are weighted based on the exponential of the negative out-of-bag error.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        y : ndarray of shape = [n_samples]</span>
<span class="sd">            The predicted values.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        If &#39;expOOB&#39; weighting is used, trees with lower out-of-bag error have a greater influence on</span>
<span class="sd">        the final prediction, potentially improving predictive performance on unseen data.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;estimators_&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The forest is not fitted yet!&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">weights</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;expOOB&quot;</span><span class="p">,</span> <span class="s2">&quot;uniform&quot;</span><span class="p">]:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="s2">&quot;uniform&quot;</span>

        <span class="c1"># Collect predictions from each tree</span>
        <span class="n">all_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;expOOB&quot;</span><span class="p">:</span>
            <span class="c1"># Use the exponential of the negative out-of-bag error as weights</span>
            <span class="n">weighted_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
            <span class="c1"># All trees have equal weight</span>
            <span class="n">weighted_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">weighted_preds</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="regressor.CustomRandomForestRegressor.fit" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Build a forest of trees from the training set (X, y).</p>
<h5 id="regressor.CustomRandomForestRegressor.fit--parameters">Parameters</h5>
<p>X : array-like or sparse matrix of shape = [n_samples, n_features]
    The training input samples.
y : array-like of shape = [n_samples] or [n_samples, n_outputs]
    The target values (real numbers).</p>
<h5 id="regressor.CustomRandomForestRegressor.fit--returns">Returns</h5>
<p>self : object
    Returns self.</p>

          <details class="quote">
            <summary>Source code in <code>regressor.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build a forest of trees from the training set (X, y).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">        The training input samples.</span>
<span class="sd">    y : array-like of shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">        The target values (real numbers).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self : object</span>
<span class="sd">        Returns self.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_bag_indices_</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">oob_indices_</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
        <span class="n">random_state</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">random_state</span>
        <span class="n">in_bag_indices</span> <span class="o">=</span> <span class="n">generate_sample_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="n">oob_indices</span> <span class="o">=</span> <span class="n">generate_unsampled_indices</span><span class="p">(</span><span class="n">random_state</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">in_bag_indices_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">in_bag_indices</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">oob_indices_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">oob_indices</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">oob_indices</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">oob_predictions</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">oob_indices</span><span class="p">])</span>
            <span class="n">oob_loss</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">oob_indices</span><span class="p">],</span> <span class="n">oob_predictions</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">oob_loss</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Normalize tree weights</span>
    <span class="n">total_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">total_weight</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">weight</span> <span class="o">/</span> <span class="n">total_weight</span> <span class="k">for</span> <span class="n">weight</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span>
        <span class="p">]</span>

    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="regressor.CustomRandomForestRegressor.predict" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Predict regression target for X using the trained forest.</p>
<h5 id="regressor.CustomRandomForestRegressor.predict--parameters">Parameters</h5>
<p>X : array-like or sparse matrix of shape = [n_samples, n_features]
    The input samples.
weights : {'uniform', 'expOOB'}, default='uniform'
    The weighting scheme to use for aggregating predictions from the individual trees.
    - 'uniform': All trees contribute equally to the final prediction.
    - 'expOOB': Trees are weighted based on the exponential of the negative out-of-bag error.</p>
<h5 id="regressor.CustomRandomForestRegressor.predict--returns">Returns</h5>
<p>y : ndarray of shape = [n_samples]
    The predicted values.</p>
<h5 id="regressor.CustomRandomForestRegressor.predict--notes">Notes</h5>
<p>If 'expOOB' weighting is used, trees with lower out-of-bag error have a greater influence on
the final prediction, potentially improving predictive performance on unseen data.</p>

          <details class="quote">
            <summary>Source code in <code>regressor.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predict regression target for X using the trained forest.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">        The input samples.</span>
<span class="sd">    weights : {&#39;uniform&#39;, &#39;expOOB&#39;}, default=&#39;uniform&#39;</span>
<span class="sd">        The weighting scheme to use for aggregating predictions from the individual trees.</span>
<span class="sd">        - &#39;uniform&#39;: All trees contribute equally to the final prediction.</span>
<span class="sd">        - &#39;expOOB&#39;: Trees are weighted based on the exponential of the negative out-of-bag error.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    y : ndarray of shape = [n_samples]</span>
<span class="sd">        The predicted values.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    If &#39;expOOB&#39; weighting is used, trees with lower out-of-bag error have a greater influence on</span>
<span class="sd">    the final prediction, potentially improving predictive performance on unseen data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;estimators_&quot;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The forest is not fitted yet!&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">weights</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;expOOB&quot;</span><span class="p">,</span> <span class="s2">&quot;uniform&quot;</span><span class="p">]:</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="s2">&quot;uniform&quot;</span>

    <span class="c1"># Collect predictions from each tree</span>
    <span class="n">all_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;expOOB&quot;</span><span class="p">:</span>
        <span class="c1"># Use the exponential of the negative out-of-bag error as weights</span>
        <span class="n">weighted_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_weights_</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">weights</span> <span class="o">==</span> <span class="s2">&quot;uniform&quot;</span><span class="p">:</span>
        <span class="c1"># All trees have equal weight</span>
        <span class="n">weighted_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">weighted_preds</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>




  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../classifier/" class="btn btn-neutral float-left" title="Custom RF Classifier"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../utils/" class="btn btn-neutral float-right" title="Utils">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../classifier/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../utils/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
